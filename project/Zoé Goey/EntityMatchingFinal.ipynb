{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sIYCZ5zX2j1Z"
   },
   "source": [
    "# Entity Matching\n",
    "\n",
    "The process of finding matching records in tables is called entity matching. It can be used for deduplication of tables or to link tables to merge the information that they contain. The question that we want to answer in this notebook is whether machine learning has something to add to traditional programming methods for entity matching and whether fancy deep learning approaches really outperform simpler machine learning models that have already been around for decades.\n",
    "\n",
    "To investigate this, we make use of two existing entity matching packages: the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/) (PRLT), which offers support for traditional programimng techniques and basic machine learning models and [DeepMatcher](https://anhaidgroup.github.io/deepmatcher/html/), a package that adopts deep learning techniques. We test both packages on  scientific bibliographic data from [ACM](https://www.acm.org/) and [DBLP](https://dblp.org/), which we downloaded from the [Database Group Leipzig ](https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution). This is an independent dataset that was not used to advertise either of the two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDTgSkwI2j1a"
   },
   "source": [
    "## Package Imports\n",
    "\n",
    "We now import the packages that we are going to use, together with our self-created module em_helper that contains some utitility functions to generate and display classification performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRaIZ9eQ2j1b"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "   import recordlinkage\n",
    "except:\n",
    "    !pip install -q recordlinkage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "try:\n",
    "  import deepmatcher as dm\n",
    "except:\n",
    "    !pip install -q deepmatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UmBFcjDhBTXv"
   },
   "source": [
    "## Setting random seeds\n",
    "We would like to keep our experiment as reproducible a possible so we start by fixing the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZsYg-IHCHjm"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oquUJKhGj0a"
   },
   "source": [
    "## Mounting Google Drive\n",
    "To get access to data, we will mount our Google Drive (comment this out when you are working on a local machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ad6V1zLvHEtN",
    "outputId": "48e77523-b3d4-4ee2-d496-db8d0e97d14b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HA_W1U64ULGi"
   },
   "source": [
    "# Setting root directory\n",
    "We now set the root directory. Note that this differs when you are running from Google Colab compared to running locally. Comment out to non-appliying option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JeHCCw-IUZuf"
   },
   "outputs": [],
   "source": [
    "# root = '.'\n",
    "root = root = '/drive/My Drive/EntityMatching/'\n",
    "import sys \n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yooauu72j1e"
   },
   "source": [
    "## Data Loading\n",
    "Having all our dependencies in place, we now load and display the two tables that contain the records that we want to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 883
    },
    "colab_type": "code",
    "id": "Qmqyp3Q72j1f",
    "outputId": "1b52aafe-840d-4532-a528-a8200b4aad6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>journals/sigmod/Mackay99</th>\n",
       "      <td>Semantic Integration of Environmental Models f...</td>\n",
       "      <td>D. Scott Mackay</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/PoosalaI96</th>\n",
       "      <td>Estimation of Query-Result Distribution and it...</td>\n",
       "      <td>Viswanath Poosala, Yannis E. Ioannidis</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/PalpanasSCP02</th>\n",
       "      <td>Incremental Maintenance for Non-Distributive A...</td>\n",
       "      <td>Themistoklis Palpanas, Richard Sidle, Hamid Pi...</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/GardarinGT96</th>\n",
       "      <td>Cost-based Selection of Path Expression Proces...</td>\n",
       "      <td>Zhao-Hui Tang, Georges Gardarin, Jean-Robert G...</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/HoelS95</th>\n",
       "      <td>Benchmarking Spatial Join Operations with Spat...</td>\n",
       "      <td>Erik G. Hoel, Hanan Samet</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journals/tods/KarpSP03</th>\n",
       "      <td>A simple algorithm for finding frequent elemen...</td>\n",
       "      <td>Scott Shenker, Christos H. Papadimitriou, Rich...</td>\n",
       "      <td>ACM Trans. Database Syst.</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/LimWV03</th>\n",
       "      <td>SASH: A Self-Adaptive Histogram Set for Dynami...</td>\n",
       "      <td>Lipyeow Lim, Min Wang, Jeffrey Scott Vitter</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journals/tods/ChakrabartiKMP02</th>\n",
       "      <td>Locally adaptive dimensionality reduction for ...</td>\n",
       "      <td>Kaushik Chakrabarti, Eamonn J. Keogh, Michael ...</td>\n",
       "      <td>ACM Trans. Database Syst.</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journals/sigmod/Snodgrass01</th>\n",
       "      <td>Chair's Message</td>\n",
       "      <td>Richard T. Snodgrass</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/LiM01</th>\n",
       "      <td>Indexing and Querying XML Data for Regular Pat...</td>\n",
       "      <td>Bongki Moon, Quanzhong Li</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2616 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            title  ...  year\n",
       "id                                                                                 ...      \n",
       "journals/sigmod/Mackay99        Semantic Integration of Environmental Models f...  ...  1999\n",
       "conf/vldb/PoosalaI96            Estimation of Query-Result Distribution and it...  ...  1996\n",
       "conf/vldb/PalpanasSCP02         Incremental Maintenance for Non-Distributive A...  ...  2002\n",
       "conf/vldb/GardarinGT96          Cost-based Selection of Path Expression Proces...  ...  1996\n",
       "conf/vldb/HoelS95               Benchmarking Spatial Join Operations with Spat...  ...  1995\n",
       "...                                                                           ...  ...   ...\n",
       "journals/tods/KarpSP03          A simple algorithm for finding frequent elemen...  ...  2003\n",
       "conf/vldb/LimWV03               SASH: A Self-Adaptive Histogram Set for Dynami...  ...  2003\n",
       "journals/tods/ChakrabartiKMP02  Locally adaptive dimensionality reduction for ...  ...  2002\n",
       "journals/sigmod/Snodgrass01                                       Chair's Message  ...  2001\n",
       "conf/vldb/LiM01                 Indexing and Querying XML Data for Regular Pat...  ...  2001\n",
       "\n",
       "[2616 rows x 4 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304586</th>\n",
       "      <td>The WASA2 object-oriented workflow management ...</td>\n",
       "      <td>Gottfried Vossen, Mathias Weske</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304587</th>\n",
       "      <td>A user-centered interface for querying distrib...</td>\n",
       "      <td>Isabel F. Cruz, Kimberly M. James</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304589</th>\n",
       "      <td>World Wide Database-integrating the Web, CORBA...</td>\n",
       "      <td>Athman Bouguettaya, Boualem Benatallah, Lily H...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304590</th>\n",
       "      <td>XML-based information mediation with MIX</td>\n",
       "      <td>Chaitan Baru, Amarnath Gupta, Bertram Lud&amp;#228...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304582</th>\n",
       "      <td>The CCUBE constraint object-oriented database ...</td>\n",
       "      <td>Alexander Brodsky, Victor E. Segal, Jia Chen, ...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672977</th>\n",
       "      <td>Dual-Buffering Strategies in Object Bases</td>\n",
       "      <td>Alfons Kemper, Donald Kossmann</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950482</th>\n",
       "      <td>Guest editorial</td>\n",
       "      <td>Philip A. Bernstein, Yannis Ioannidis, Raghu R...</td>\n",
       "      <td>The VLDB Journal &amp;mdash; The International Jou...</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672980</th>\n",
       "      <td>GraphDB: Modeling and Querying Graphs in Datab...</td>\n",
       "      <td>Ralf Hartmut G&amp;#252;ting</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945741</th>\n",
       "      <td>Review of The data warehouse toolkit: the comp...</td>\n",
       "      <td>Alexander A. Anisimov</td>\n",
       "      <td>ACM SIGMOD Record</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672979</th>\n",
       "      <td>Bulk Loading into an OODB: A Performance Study</td>\n",
       "      <td>Janet L. Wiener, Jeffrey F. Naughton</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2294 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  ...  year\n",
       "id                                                         ...      \n",
       "304586  The WASA2 object-oriented workflow management ...  ...  1999\n",
       "304587  A user-centered interface for querying distrib...  ...  1999\n",
       "304589  World Wide Database-integrating the Web, CORBA...  ...  1999\n",
       "304590           XML-based information mediation with MIX  ...  1999\n",
       "304582  The CCUBE constraint object-oriented database ...  ...  1999\n",
       "...                                                   ...  ...   ...\n",
       "672977          Dual-Buffering Strategies in Object Bases  ...  1994\n",
       "950482                                    Guest editorial  ...  2003\n",
       "672980  GraphDB: Modeling and Querying Graphs in Datab...  ...  1994\n",
       "945741  Review of The data warehouse toolkit: the comp...  ...  2003\n",
       "672979     Bulk Loading into an OODB: A Performance Study  ...  1994\n",
       "\n",
       "[2294 rows x 4 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_left = os.path.join(root, 'Data/DBLP2.csv')\n",
    "data_right = os.path.join(root, 'Data/ACM.csv')\n",
    "left_df = pd.read_csv(data_left, encoding='cp1252')\n",
    "right_df = pd.read_csv(data_right, encoding='cp1252')\n",
    "left_df.set_index('id',inplace=True)\n",
    "right_df.set_index('id',inplace=True)\n",
    "display(left_df)\n",
    "display(right_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zu5aucGI2j1i"
   },
   "source": [
    "# Defining Candidate Links\n",
    "To translate the matching problem into a standard classification problem, we first form all possible combinations of rows in the DBLP table and rows in the ACM table. In other words, we form the Cartesian product of the indices of these tables. The DBLP table has 2616 rows and the the ACM table has 2294 rows, so this results in a collection of 2616 x 2294 = 6001104 combinations. \n",
    "\n",
    "The size of this set of possible links now places us for a dilemma:\n",
    "1. We would like to use as much of the data as possible to reach a sound conclusion.\n",
    "2. DeepMatcher is not that fast and will take ages to process such a  large dataset.\n",
    "\n",
    "To solve this dilemma, we apply a technique that is commonly used in entity matching: _blocking_. By using a simple heuristic, we filter out row combinations, of which we can be (almost) certain that they do not match. By blocking these row combinations, we end up with a considerably smaller set of candidate links that is much easier to manage. \n",
    "\n",
    "To perform blocking, we make use of the string comparison capabilities of the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/). We block all combinations with a [Lehvenstein-distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)-based similarity measure that is smaller than 0.3, where 0 indicates a complete mismatch and 1 indicates identical strings. By doing this, we reduce the number of candidate links to 162894, which amounts to a data reduction of about 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "95KahB3e2j1l",
    "outputId": "bab8fb62-c218-4462-d6d6-f91fd9868d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:recordlinkage:indexing - performance warning - A full index can result in large number of record pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:recordlinkage:indexing - performance warning - A full index can result in large number of record pairs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Number of candidate links in Cartesian product: 6001104'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Number of candidate links after blocking: 162894'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Full link\n",
    "indexer = recordlinkage.Index()\n",
    "indexer.full()\n",
    "links = indexer.index(left_df, right_df)\n",
    "\n",
    "# Comparison step for blocking\n",
    "compare_cl = recordlinkage.Compare()\n",
    "compare_cl.string('title', 'title', threshold=0.3, label='title')\n",
    "features = compare_cl.compute(links, left_df, right_df)\n",
    "\n",
    "# Blocking\n",
    "candidate_links = links[features.sum(axis=1) > 0]\n",
    "\n",
    "# Show data reduction\n",
    "display(\"Number of candidate links in Cartesian product: {}\".format(len(links)))\n",
    "display(\"Number of candidate links after blocking: {}\".format(len(candidate_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7hxQKvm2j1o"
   },
   "source": [
    "## Inspection of blocking effects\n",
    "Since blocking might have the unwanted effect of throwing away a considerable number of true mtaches, we briefly verify whether this is the case. We load the data set containing the true links and see how much of these links are still present in the blocked data set. It turns out that only 8 of the 2224 true matches are discarded by the blocking procesdure, which amounts to 0.36%. This seems to be a defendable sacrifice to make for the 97% data reduction that is achieved by blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "8SkYCF0N2j1p",
    "outputId": "47ae0468-3ea8-48d4-ab99-2400cfad4ce7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Number of true matches in full set: 2216'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Number of true matches in blocked set: 2224'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_match = os.path.join(root, 'Data/DBLP-ACM_perfectMapping.csv')\n",
    "matches = pd.read_csv(data_match, encoding='cp1252')\n",
    "matches_tuples = list(matches.itertuples(index=False, name=None)) \n",
    "candidate_links_tuples = candidate_links.tolist()\n",
    "intersection = set(candidate_links_tuples).intersection(matches_tuples)\n",
    "display(\"Number of true matches in full set: {}\".format(len(intersection)))\n",
    "display(\"Number of true matches in blocked set: {}\".format(len(matches_tuples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-sNUloI2j1r"
   },
   "source": [
    "## Data preparation for DeepMatcher\n",
    "To use the DeepMatcher classifier, we need to transform our data into something that [DeepMatcher](https://anhaidgroup.github.io/deepmatcher/html/) can understand. DeepMatcher takes in a table with record pairs of the two tables, which need to be matched. The records of the first table  are marked by adding the prefix \"left_\" to their feature names, and the records of the second table are marked by adding the prefix \"right_\". The target variable that indicates whether a pair forms a match is called \"label\" and takes on the values 0 (no match) or 1 (match). To prevent unwanted correlations between the subsequent rows, we additionally shuffle the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "id": "e5fdrAuz2j1s",
    "outputId": "63d7f6ad-94aa-4d41-b096-e9de7350910c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>left_title</th>\n",
       "      <th>left_authors</th>\n",
       "      <th>left_venue</th>\n",
       "      <th>left_year</th>\n",
       "      <th>right_title</th>\n",
       "      <th>right_authors</th>\n",
       "      <th>right_venue</th>\n",
       "      <th>right_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5962337</th>\n",
       "      <td>0</td>\n",
       "      <td>Indexing Images in Oracle8i</td>\n",
       "      <td>Melliyal Annamalai, Samuel DeFazio, Rajiv Chopra</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>2000</td>\n",
       "      <td>Standards in practice</td>\n",
       "      <td>Andrew Eisenberg, Jim Melton</td>\n",
       "      <td>ACM SIGMOD Record</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063086</th>\n",
       "      <td>0</td>\n",
       "      <td>Illuminating the Dark Side of Web Services</td>\n",
       "      <td>Michael L. Brodie</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2003</td>\n",
       "      <td>Information warfare and security</td>\n",
       "      <td>H. V. Jagadish</td>\n",
       "      <td>ACM SIGMOD Record</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193273</th>\n",
       "      <td>0</td>\n",
       "      <td>Locating and accessing data repositories with ...</td>\n",
       "      <td>George A. Mihaila, Louiqa Raschid, Anthony Tom...</td>\n",
       "      <td>VLDB J.</td>\n",
       "      <td>2002</td>\n",
       "      <td>Logical logging to extend recovery to new domains</td>\n",
       "      <td>David Lomet, Mark Tuttle</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5707520</th>\n",
       "      <td>1</td>\n",
       "      <td>Storing Semistructured Data with STORED</td>\n",
       "      <td>Alin Deutsch, Mary F. Fernandez, Dan Suciu</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1999</td>\n",
       "      <td>Storing semistructured data with STORED</td>\n",
       "      <td>Alin Deutsch, Mary Fernandez, Dan Suciu</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100816</th>\n",
       "      <td>0</td>\n",
       "      <td>From Structured Documents to Novel Query Facil...</td>\n",
       "      <td>Sophie Cluet, Michel Scholl, Serge Abiteboul, ...</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1994</td>\n",
       "      <td>The Fittest Survives: An Adaptive Approach to ...</td>\n",
       "      <td>Hongjun Lu, Kian-Lee Tan, Son Dao</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597271</th>\n",
       "      <td>0</td>\n",
       "      <td>Parallel Evaluation of Multi-Join Queries</td>\n",
       "      <td>Jan Flokstra, Annita N. Wilschut, Peter M. G. ...</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1995</td>\n",
       "      <td>Orthogonal optimization of subqueries and aggr...</td>\n",
       "      <td>C&amp;#233;sar Galindo-Legaria, Milind Joshi</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3545376</th>\n",
       "      <td>0</td>\n",
       "      <td>Dynamic Information Visualization</td>\n",
       "      <td>Yannis E. Ioannidis</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>1996</td>\n",
       "      <td>MineSet(tm): A System for High-End Data Mining...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5587905</th>\n",
       "      <td>0</td>\n",
       "      <td>A Language Based Multidatabase System</td>\n",
       "      <td>Konrad Schwarz, eva Kühn, Thomas Tschernko</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1994</td>\n",
       "      <td>Transparent mid-tier database caching in SQL s...</td>\n",
       "      <td>Per-&amp;#197;ke Larson, Jonathan Goldstein, Jingr...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374589</th>\n",
       "      <td>0</td>\n",
       "      <td>Similarity Search for Adaptive Ellipsoid Queri...</td>\n",
       "      <td>Yasushi Sakurai, Shunsuke Uemura, Masatoshi Yo...</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2001</td>\n",
       "      <td>Efficient and Effective Clustering Methods for...</td>\n",
       "      <td>Raymond T. Ng, Jiawei Han</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576185</th>\n",
       "      <td>0</td>\n",
       "      <td>Query Optimization at the Crossroads (Panel)</td>\n",
       "      <td>Surajit Chaudhuri</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1997</td>\n",
       "      <td>Query optimization for selections using bitmaps</td>\n",
       "      <td>Ming-Chuan Wu</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162894 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  ... right_year\n",
       "id              ...           \n",
       "5962337      0  ...       1998\n",
       "3063086      0  ...       2001\n",
       "2193273      0  ...       1999\n",
       "5707520      1  ...       1999\n",
       "4100816      0  ...       1995\n",
       "...        ...  ...        ...\n",
       "3597271      0  ...       2001\n",
       "3545376      0  ...       1996\n",
       "5587905      0  ...       2003\n",
       "4374589      0  ...       1994\n",
       "1576185      0  ...       1999\n",
       "\n",
       "[162894 rows x 9 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_df_renamed =left_df.reset_index().add_prefix('left_')\n",
    "left_df_renamed['join'] = 1\n",
    "right_df_renamed =right_df.reset_index().add_prefix('right_')\n",
    "right_df_renamed['join'] = 1\n",
    "final_df_full = pd.merge(left_df_renamed, right_df_renamed, on=\"join\")\n",
    "final_df_full.insert(0,'label',0)\n",
    "final_df_full['combined_index'] = list(zip(final_df_full.left_id, final_df_full.right_id))\n",
    "final_df_full.loc[final_df_full['combined_index'].isin(matches_tuples),'label'] = 1\n",
    "final_df_blocked = final_df_full.loc[final_df_full['combined_index'].isin(candidate_links)]\n",
    "final_df = final_df_blocked.drop(columns=['combined_index','left_id','right_id','join'])\n",
    "final_df.index.name = 'id'\n",
    "final_df = final_df.sample(frac=1)\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OED0LaZw2j1u"
   },
   "source": [
    "## Creating Sets for Training, Validation and Testing\n",
    "Now that our dataset is the right format, we create a train, validation and test set by applying a 60:20:20 split. The resulting datasets are saved to csv-files inside the Data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MiSSN1AM2j1v"
   },
   "outputs": [],
   "source": [
    "train_split,validate_split,test_split = np.split(final_df, [int(.6 * len(final_df)), int(.8 * len(final_df))])\n",
    "train_file = os.path.join(root,'Data/train_block.csv')\n",
    "validate_file = os.path.join(root,'Data/validate_block.csv')\n",
    "test_file = os.path.join(root,'Data/test_block.csv')\n",
    "train_split.to_csv(train_file)\n",
    "validate_split.to_csv(validate_file)\n",
    "test_split.to_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMZXii512j1x"
   },
   "source": [
    "## Deepmatcher Data Preprocessing and Feature Engineering\n",
    "We now let [DeepMatcher](https://anhaidgroup.github.io/deepmatcher/html/) apply its standard data preprecessing and feature engineering procedure. Due to the time limits of the Project Challenge, we have not delved deeper into this, but rather used it as a black box. To get a rough idea of what [DeepMatcher](https://anhaidgroup.github.io/deepmatcher/html/) does behind the scenes, we display the still human-interpretable raw data table, which holds the result of text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 980
    },
    "colab_type": "code",
    "id": "T6HyXG0v2j1y",
    "outputId": "0bdf28d1-1a75-4e4a-e16b-0d5adf419178"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading and processing data from \"/drive/My Drive/EntityMatching/Data/train_block.csv\"\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \"/drive/My Drive/EntityMatching/Data/validate_block.csv\"\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \"/drive/My Drive/EntityMatching/Data/test_block.csv\"\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Building vocabulary\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n",
      "\n",
      "Computing principal components\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>left_title</th>\n",
       "      <th>left_authors</th>\n",
       "      <th>left_venue</th>\n",
       "      <th>left_year</th>\n",
       "      <th>right_title</th>\n",
       "      <th>right_authors</th>\n",
       "      <th>right_venue</th>\n",
       "      <th>right_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5962337</td>\n",
       "      <td>0</td>\n",
       "      <td>indexing images in oracle8i</td>\n",
       "      <td>melliyal annamalai , samuel defazio , rajiv ch...</td>\n",
       "      <td>sigmod conference</td>\n",
       "      <td>2000</td>\n",
       "      <td>standards in practice</td>\n",
       "      <td>andrew eisenberg , jim melton</td>\n",
       "      <td>acm sigmod record</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3063086</td>\n",
       "      <td>0</td>\n",
       "      <td>illuminating the dark side of web services</td>\n",
       "      <td>michael l. brodie</td>\n",
       "      <td>vldb</td>\n",
       "      <td>2003</td>\n",
       "      <td>information warfare and security</td>\n",
       "      <td>h. v. jagadish</td>\n",
       "      <td>acm sigmod record</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2193273</td>\n",
       "      <td>0</td>\n",
       "      <td>locating and accessing data repositories with ...</td>\n",
       "      <td>george a. mihaila , louiqa raschid , anthony t...</td>\n",
       "      <td>vldb j .</td>\n",
       "      <td>2002</td>\n",
       "      <td>logical logging to extend recovery to new domains</td>\n",
       "      <td>david lomet , mark tuttle</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5707520</td>\n",
       "      <td>1</td>\n",
       "      <td>storing semistructured data with stored</td>\n",
       "      <td>alin deutsch , mary f. fernandez , dan suciu</td>\n",
       "      <td>sigmod conference</td>\n",
       "      <td>1999</td>\n",
       "      <td>storing semistructured data with stored</td>\n",
       "      <td>alin deutsch , mary fernandez , dan suciu</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4100816</td>\n",
       "      <td>0</td>\n",
       "      <td>from structured documents to novel query facil...</td>\n",
       "      <td>sophie cluet , michel scholl , serge abiteboul...</td>\n",
       "      <td>sigmod conference</td>\n",
       "      <td>1994</td>\n",
       "      <td>the fittest survives : an adaptive approach to...</td>\n",
       "      <td>hongjun lu , kian-lee tan , son dao</td>\n",
       "      <td>very large data bases</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97731</th>\n",
       "      <td>3346855</td>\n",
       "      <td>0</td>\n",
       "      <td>highly concurrent cache consistency for indice...</td>\n",
       "      <td>michael j. carey , markos zaharioudakis</td>\n",
       "      <td>sigmod conference</td>\n",
       "      <td>1997</td>\n",
       "      <td>new concurrency control algorithms for accessi...</td>\n",
       "      <td>v. w. setzer , andrea zisman</td>\n",
       "      <td>very large data bases</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97732</th>\n",
       "      <td>5201113</td>\n",
       "      <td>0</td>\n",
       "      <td>indexing large metric spaces for similarity se...</td>\n",
       "      <td>z. meral özsoyoglu , tolga bozkaya</td>\n",
       "      <td>acm trans . database syst .</td>\n",
       "      <td>1999</td>\n",
       "      <td>estimating page fetches for index scans with f...</td>\n",
       "      <td>arun swami , k. bernhard schiefer</td>\n",
       "      <td>the vldb journal &amp; mdash ; the international j...</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97733</th>\n",
       "      <td>3473534</td>\n",
       "      <td>0</td>\n",
       "      <td>the sr-tree : an index structure for high-dime...</td>\n",
       "      <td>norio katayama , shin'ichi satoh</td>\n",
       "      <td>sigmod conference</td>\n",
       "      <td>1997</td>\n",
       "      <td>the onion technique : indexing for linear opti...</td>\n",
       "      <td>yuan-chi chang , lawrence bergman , vittorio c...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97734</th>\n",
       "      <td>3464565</td>\n",
       "      <td>0</td>\n",
       "      <td>algorithms for mining association rules for bi...</td>\n",
       "      <td>kunikazu yoda , hirofumi matsuzawa , takeshi t...</td>\n",
       "      <td>vldb</td>\n",
       "      <td>1998</td>\n",
       "      <td>data model for extensible support of explicit ...</td>\n",
       "      <td>joan peckham , bonnie mackellar , michael doherty</td>\n",
       "      <td>the vldb journal &amp; mdash ; the international j...</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97735</th>\n",
       "      <td>4756082</td>\n",
       "      <td>0</td>\n",
       "      <td>in-context peer-to-peer information filtering ...</td>\n",
       "      <td>aris m. ouksel</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>2003</td>\n",
       "      <td>agents , trust , and information access on the...</td>\n",
       "      <td>tim finin , anupam joshi</td>\n",
       "      <td>acm sigmod record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97736 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  ...  right_year\n",
       "0      5962337  ...        1998\n",
       "1      3063086  ...        2001\n",
       "2      2193273  ...        1999\n",
       "3      5707520  ...        1999\n",
       "4      4100816  ...        1995\n",
       "...        ...  ...         ...\n",
       "97731  3346855  ...        1994\n",
       "97732  5201113  ...        1995\n",
       "97733  3473534  ...        2000\n",
       "97734  3464565  ...        1995\n",
       "97735  4756082  ...        2002\n",
       "\n",
       "[97736 rows x 10 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, validate, test = dm.data.process(\n",
    "    path='.',\n",
    "    left_prefix='left_',\n",
    "    right_prefix='right_',\n",
    "    label_attr='label',\n",
    "    id_attr='id',\n",
    "    cache=None,\n",
    "    train=train_file,\n",
    "    validation=validate_file,\n",
    "    test=test_file)\n",
    "train_table = train.get_raw_table()\n",
    "display(train_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LXmHS-Tn2j10"
   },
   "source": [
    "## DeepMatcher Model Training\n",
    "We are now ready to train our DeepMatcher model. We take over the values of he DeepMatcher [\"Getting started\"-notebook](https://github.com/anhaidgroup/deepmatcher/blob/master/examples/getting_started.ipynb), but make a couple of small adjustments. Knowing that an epoch will take about half an hour on Google Colab and 2 hours on our local machine, we limit the number of epochs to 8 instead of 10, and since we have not seen any substantial benefit of weighting in smaller experiments, we practically disable it by setting pos_neg_ratio = 1.\n",
    "\n",
    "The training function prints the performance metric F1-measure, precision and recall for both the training and the validation set at each epoch and uses the F1-measure on the validation set to select the best model. The F1-measure is more suitable than accuracy in thie case, because even in the blocked dataset, the fraction of true matches is very small (about 1.4% )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uPHwz0wq2j11",
    "outputId": "8e4913c5-71ce-44dd-fb12-c9aa31b2209c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Number of trainable parameters: 9210006\n",
      "===>  TRAIN Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2352: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:23:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time: 1140.1 | Load Time:  245.1 || F1:  92.86 | Prec:  95.68 | Rec:  90.20 || Ex/s:  70.56\n",
      "\n",
      "===>  EVAL Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:  171.0 | Load Time:   80.6 || F1:  97.04 | Prec:  97.52 | Rec:  96.56 || Ex/s: 129.51\n",
      "\n",
      "* Best F1: tensor(97.0370, device='cuda:0')\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:22:37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time: 1118.7 | Load Time:  239.8 || F1:  97.76 | Prec:  98.25 | Rec:  97.29 || Ex/s:  71.94\n",
      "\n",
      "===>  EVAL Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:  173.1 | Load Time:   80.7 || F1:  97.79 | Prec:  97.79 | Rec:  97.79 || Ex/s: 128.37\n",
      "\n",
      "* Best F1: tensor(97.7887, device='cuda:0')\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:23:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time: 1140.9 | Load Time:  242.9 || F1:  98.60 | Prec:  99.01 | Rec:  98.19 || Ex/s:  70.63\n",
      "\n",
      "===>  EVAL Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:  173.2 | Load Time:   81.2 || F1:  98.02 | Prec:  98.75 | Rec:  97.30 || Ex/s: 128.05\n",
      "\n",
      "* Best F1: tensor(98.0198, device='cuda:0')\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:22:55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time: 1135.1 | Load Time:  240.9 || F1:  98.79 | Prec:  98.79 | Rec:  98.79 || Ex/s:  71.03\n",
      "\n",
      "===>  EVAL Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:  173.6 | Load Time:   80.3 || F1:  98.01 | Prec:  99.00 | Rec:  97.05 || Ex/s: 128.28\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:22:57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time: 1137.2 | Load Time:  240.8 || F1:  98.94 | Prec:  99.24 | Rec:  98.64 || Ex/s:  70.93\n",
      "\n",
      "===>  EVAL Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:  175.5 | Load Time:   80.8 || F1:  98.01 | Prec:  99.00 | Rec:  97.05 || Ex/s: 127.10\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:22:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time: 1131.3 | Load Time:  239.9 || F1:  99.36 | Prec:  99.40 | Rec:  99.32 || Ex/s:  71.28\n",
      "\n",
      "===>  EVAL Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:  170.9 | Load Time:   79.7 || F1:  96.96 | Prec:  95.91 | Rec:  98.03 || Ex/s: 130.01\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:22:48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time: 1129.7 | Load Time:  239.5 || F1:  99.40 | Prec:  99.32 | Rec:  99.47 || Ex/s:  71.38\n",
      "\n",
      "===>  EVAL Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:  174.0 | Load Time:   80.4 || F1:  98.39 | Prec:  99.50 | Rec:  97.30 || Ex/s: 128.06\n",
      "\n",
      "* Best F1: tensor(98.3851, device='cuda:0')\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:22:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time: 1135.0 | Load Time:  239.6 || F1:  99.59 | Prec:  99.62 | Rec:  99.55 || Ex/s:  71.10\n",
      "\n",
      "===>  EVAL Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:  169.6 | Load Time:   79.0 || F1:  98.00 | Prec:  99.49 | Rec:  96.56 || Ex/s: 131.08\n",
      "\n",
      "---------------------\n",
      "\n",
      "Loading best model...\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "model_file = os.path.join(root,'Results/model_dblp_acm_block.pth')\n",
    "model = dm.MatchingModel(attr_summarizer='hybrid')\n",
    "f1_dm_val = model.run_train(\n",
    "    train,\n",
    "    validate,\n",
    "    epochs=8,\n",
    "    batch_size=16,\n",
    "    best_save_path= model_file,\n",
    "    pos_neg_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p8ebQnTk2j14"
   },
   "source": [
    "## DeepMatcher Testing\n",
    "We are now curious to see what the selected model does on the test set. We see that compared to the training and validation results, there is no serious drop in the F1-value, so we are reasonably confident that our model is not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Xlb8Gf0E2j15",
    "outputId": "f3b66538-14c3-4a45-8781-341daedbace9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  EVAL Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:   89.5 | Load Time:   85.7 || F1:  99.07 | Prec:  99.17 | Rec:  98.96 || Ex/s: 186.03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_dm_test = model.run_eval(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I223AbWc2j17"
   },
   "source": [
    "##  PRLT Feature Engineering\n",
    "We are now ready to switch to the the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/), which uses a simpler approach to feature engineering. Basically, we fill a vector with zeros of ones that indicate whether a record pair matches with respect to a certain column. So for each of the columns in the left/right tables, we define how similar they need to be to be matching. All the values of the per-column-matches are then concatenated to a global feature vector of zeros and ones, where  an all zero vector means that the pair matches in none of the columns, an an all-one-vector indicates a match with respect to every column. This vector of zeros and ones is then fed to the various classifier models.\n",
    "\n",
    "To define when we call two colum values matching, we let ourselves be inspired by the [example](https://recordlinkage.readthedocs.io/en/latest/notebooks/link_two_dataframes.html) that the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/) provides in its introductory text. Here most of the column matches are defined in terms of a similarity that is based on the [Jaro Winkler distance](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance). The only small adaptation that we make is in the choice of the similarity threshold value for \"venue\", because a short inspection of the data indicates that matching records tend to agree less with respect to \"venue\" than they do with respect to \"title\" or 'author\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mQVssCt82j18",
    "outputId": "ee6da173-aa13-4e84-f8d3-c57a95ddfdfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compare>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column-based matching criteria that define the feature vector\n",
    "compare_cl = recordlinkage.Compare()\n",
    "compare_cl.string('title', 'title', method='jarowinkler', threshold=0.85,label = 'title')\n",
    "compare_cl.string('authors', 'authors', method='jarowinkler', threshold=0.85,label = 'authors')\n",
    "compare_cl.exact('year', 'year', label='year')\n",
    "compare_cl.string('venue', 'venue', method='jarowinkler', threshold=0.65,label = 'authors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQqc5K4P2j1_"
   },
   "source": [
    "## PRLT Data Loading and Feature Computation\n",
    "We now load our previously saved train, test andvalidation sets from file. Ironically, to feed the data to the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/) methods, we need to split up the joined table into two separate tables and provide a list of links indicating which record combinations need to be considered in the computation of the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2zFyPwb2j2A"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "true_links = {}\n",
    "features = {}\n",
    "validation_dict = {}\n",
    "datasets= {'train':train_file, \n",
    "'test':test_file,\n",
    "'validation':validate_file}\n",
    "\n",
    "for key in datasets:\n",
    "    df = pd.read_csv(datasets[key])\n",
    "    nof_cols = int((df.shape[1] - 2)/2)\n",
    "    dfA = df.iloc[:,2:nof_cols + 2] # left table\n",
    "    dfB = df.iloc[:,nof_cols + 2:df.shape[1]] # right table\n",
    "    # Delete 'left_' and 'right_' prefixes from column names\n",
    "    dfA.rename(columns={c:c[5:] for c in dfA.columns },inplace=True) \n",
    "    dfB.rename(columns={c:c[6:] for c in dfB.columns },inplace=True)\n",
    "\n",
    "    tuples = [(i,i) for i in range(len(df)) if df.iloc[i]['label'] == 1]\n",
    "    true_links[key] = pd.MultiIndex.from_tuples(tuples)\n",
    "\n",
    "    tuples_full = [(i,i) for i in range(len(df))]\n",
    "    candidate_links = pd.MultiIndex.from_tuples(tuples_full)\n",
    "    # Final features (used in all methods)\n",
    "    features[key] = compare_cl.compute(candidate_links, dfA, dfB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjjrfqiWlUtP"
   },
   "source": [
    "## PRLT Helper functions\n",
    "We now define some helper functions used for the computation and printing of the permonce metrics of the PRLT results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnV7UBlJkx5O"
   },
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "def print_results(name, table):\n",
    "    print(\"\".join(['*' for x in range(len(name) + 1)]))\n",
    "    print('{}'.format(name))\n",
    "    print(\"\".join(['*' for x in range(len(name) + 1)]))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(table['confusion_matrix'])\n",
    "    print(\"Accuracy: {}\".format(table['accuracy']))\n",
    "    print(\"Recall: {}\".format(table['recall']))\n",
    "    print(\"F-score: {}\".format(table['f-score']))\n",
    "    print('\\n')\n",
    "    \n",
    "def performance_metrics(true_links, result, set_size):\n",
    "    validation = {}\n",
    "    validation['confusion_matrix'] = recordlinkage.confusion_matrix(true_links, \n",
    "    result, set_size)\n",
    "    validation['accuracy'] = recordlinkage.accuracy(true_links, result, \n",
    "    len(features['validation']))\n",
    "    validation['precision'] = recordlinkage.precision(true_links, result)\n",
    "    validation['recall'] = recordlinkage.recall(true_links, result)\n",
    "    validation['f-score'] = recordlinkage.fscore(true_links, result)\n",
    "    return validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jzyh3Vdo2j2C"
   },
   "source": [
    "## PRLT Model Training\n",
    "We are now ready to train the various models included in the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/). There are three super vised methods (Logistic regression, Naive Bayes and Support vector machines) and two unsupervised methods (K-Means and [ECM](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.3828&rep=rep1&type=pdf)). We also add a simple heuristic that we label as \"Hand-tuned\". This basically looks whether records match on the majority of columns (more ones than zeroes in the feature vector) and classifies a pair as a match if this is the case. We assume that this heuristic as a proxy for traditional programming methods as implemented in tools like Power BI, since these are also based on string edit distances. \n",
    "\n",
    "The results of the heuristic and the various models on the validation set is displayed below. As we can see, all the supervised  trained models clearly outperform the simple heuristic, whereas the best [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/) model only slightly underperforms the much more complicated and time-consuming [DeepMatcher](https://anhaidgroup.github.io/deepmatcher/html/) model. Interestingly, the unsupervised ECM model also is a significant gain compared to the simple heuristic, indicating that also without labels, a machine learning approach can be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8Gw7zTnM2j2C",
    "outputId": "8845081c-19cd-4806-d567-0953cfef7466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********\n",
      "Hand-tuned\n",
      "***********\n",
      "Confusion matrix:\n",
      "[[  244   163]\n",
      " [   27 32145]]\n",
      "Accuracy: 0.9941680223456828\n",
      "Recall: 0.5995085995085995\n",
      "F-score: 0.71976401179941\n",
      "\n",
      "\n",
      "********************\n",
      "Logistic regression\n",
      "********************\n",
      "Confusion matrix:\n",
      "[[  392    15]\n",
      " [   30 32142]]\n",
      "Accuracy: 0.9986187421345039\n",
      "Recall: 0.9631449631449631\n",
      "F-score: 0.9457177322074789\n",
      "\n",
      "\n",
      "************\n",
      "Naive Bayes\n",
      "************\n",
      "Confusion matrix:\n",
      "[[  395    12]\n",
      " [   31 32141]]\n",
      "Accuracy: 0.9986801313729703\n",
      "Recall: 0.9705159705159705\n",
      "F-score: 0.9483793517406962\n",
      "\n",
      "\n",
      "***********************\n",
      "Support vector machine\n",
      "***********************\n",
      "Confusion matrix:\n",
      "[[  392    15]\n",
      " [   30 32142]]\n",
      "Accuracy: 0.9986187421345039\n",
      "Recall: 0.9631449631449631\n",
      "F-score: 0.9457177322074789\n",
      "\n",
      "\n",
      "********\n",
      "K-means\n",
      "********\n",
      "Confusion matrix:\n",
      "[[  407     0]\n",
      " [ 3467 28705]]\n",
      "Accuracy: 0.8935817551183277\n",
      "Recall: 1.0\n",
      "F-score: 0.190142490072413\n",
      "\n",
      "\n",
      "****\n",
      "ECM\n",
      "****\n",
      "Confusion matrix:\n",
      "[[  400     7]\n",
      " [   85 32087]]\n",
      "Accuracy: 0.9971760950305412\n",
      "Recall: 0.9828009828009828\n",
      "F-score: 0.8968609865470852\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers= {\n",
    "'Hand-tuned':None,\n",
    "'Logistic regression':recordlinkage.LogisticRegressionClassifier(),\n",
    "'Naive Bayes': recordlinkage.NaiveBayesClassifier(),\n",
    "'Support vector machine': recordlinkage.SVMClassifier(),\n",
    "'K-means': recordlinkage.KMeansClassifier(),\n",
    "'ECM': recordlinkage.ECMClassifier()}\n",
    "\n",
    "for key in classifiers:\n",
    "    validation_dict[key] = {}\n",
    "    if key == 'Hand-tuned':\n",
    "        # Immediate prediction\n",
    "        result = features['validation'][features['validation'].sum(axis=1) > 2].index\n",
    "    else:\n",
    "        # Training \n",
    "        if key == 'ECM' or key == 'K-means':\n",
    "            classifiers[key].fit(features['train']) \n",
    "        else:\n",
    "            classifiers[key].fit(features['train'], true_links['train'])\n",
    "        # Predict the match status for all test record pairs\n",
    "        result =  classifiers[key].predict(features['validation'])\n",
    "        \n",
    "    # Validate\n",
    "    validation_dict[key] = performance_metrics(true_links['validation'], result, len(features['validation']))\n",
    "    \n",
    "    #Print results\n",
    "    print_results(key, validation_dict[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFy1LPfw2j2F"
   },
   "source": [
    "## PRLT Testing\n",
    "We select the best model based on the F1-measure on the vaildation set and give it the test set to classify. The result is displayed below. Notice that the result from DeepMatcher is only slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "IgOKwodR2j2F",
    "outputId": "b18c773c-e63d-4fbd-e827-aeb5bac227f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Selected model (Naive Bayes) on test set\n",
      "*****************************************\n",
      "Confusion matrix:\n",
      "[[  464    19]\n",
      " [   28 32068]]\n",
      "Accuracy: 0.9985573528960373\n",
      "Recall: 0.9606625258799172\n",
      "F-score: 0.9517948717948718\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_scores = {key:validation_dict[key]['f-score'] for key in validation_dict}\n",
    "best_model = max(f_scores, key = f_scores.get) \n",
    "if best_model == 'Hand-tuned':\n",
    "    result = features['test'][features['test'].sum(axis=1) > 2].index\n",
    "else:\n",
    "    result = classifiers[best_model].predict(features['test'])\n",
    "test_dict =  performance_metrics(true_links['test'], result, len(features['test']))\n",
    "print_results(\"Selected model ({}) on test set\".format(best_model), test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2PPpuA2jmDMA"
   },
   "source": [
    "## Result summary\n",
    "To make a comparison between the models easier, we print a table with all the F1-measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "D-wKo5MXmXpe",
    "outputId": "fb5889f7-3bd8-44c1-e5fa-6ed48cf79dd2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row0_col0 {\n",
       "            background: white;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row0_col1 {\n",
       "            background: white;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row1_col0 {\n",
       "            background: white;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row1_col1 {\n",
       "            background: white;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row2_col0 {\n",
       "            background: lightgreen;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row2_col1 {\n",
       "            background: lightgreen;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row3_col0 {\n",
       "            background: white;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row3_col1 {\n",
       "            background: white;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row4_col0 {\n",
       "            background: white;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row4_col1 {\n",
       "            background: white;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row5_col0 {\n",
       "            background: white;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row5_col1 {\n",
       "            background: white;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row6_col0 {\n",
       "            background: lightgreen;\n",
       "        }    #T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row6_col1 {\n",
       "            background: lightgreen;\n",
       "        }</style><table id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >F1 validation</th>        <th class=\"col_heading level0 col1\" >F1 test</th>    </tr>    <tr>        <th class=\"index_name level0\" >Model</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >Hand-tuned</th>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0.719764</td>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row0_col1\" class=\"data row0 col1\" ></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >Logistic regression</th>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row1_col0\" class=\"data row1 col0\" >0.945718</td>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row1_col1\" class=\"data row1 col1\" ></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >Naive Bayes</th>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row2_col0\" class=\"data row2 col0\" >0.948379</td>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.951795</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >Support vector machine</th>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row3_col0\" class=\"data row3 col0\" >0.945718</td>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row3_col1\" class=\"data row3 col1\" ></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >K-means</th>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row4_col0\" class=\"data row4 col0\" >0.190142</td>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row4_col1\" class=\"data row4 col1\" ></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >ECM</th>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row5_col0\" class=\"data row5 col0\" >0.896861</td>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row5_col1\" class=\"data row5 col1\" ></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >DeepMatcher</th>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row6_col0\" class=\"data row6 col0\" >0.983851</td>\n",
       "                        <td id=\"T_89f20ea8_fb8f_11ea_afe2_0242ac1c0002row6_col1\" class=\"data row6 col1\" >0.990674</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdca6206f28>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_names = list(classifiers.keys())\n",
    "data = {'Model': model_names + ['DeepMatcher'],\n",
    "        'F1 validation': [validation_dict[n]['f-score'] for n in model_names] + [f1_dm_val.item()/100],\n",
    "       'F1 test':[test_dict['f-score'] if n == best_model else '' for n in model_names] + [f1_dm_test.item()/100]}\n",
    "df_summary = pd.DataFrame (data,columns = ['Model','F1 validation','F1 test'])\n",
    "df_summary.set_index('Model', inplace=True)\n",
    "model_selection = ['DeepMatcher' ,best_model]\n",
    "df_summary = df_summary.style.apply(lambda x: ['background:lightgreen' if x in model_selection else 'background:white' for x in df_summary.index])\n",
    "display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eq617iaq2j2I"
   },
   "source": [
    "## Conclusion\n",
    "Our analysis seems to indicate that machine learning approaches can be beneficial in the context of entity matching, but that the question whether deep learning really outperforms more traditional M models can be debated. Much will probably depend on the nature of the dataset, and the amount of time that one wishes to invest in labeling data and training the model. An interesting observation is that at least on our not too complicated dataset, the unsupervised ECM method also shows a very decent performance, which indicates that one can already gain something from machine learning without labeling the data.\n",
    "\n",
    "Further research could be geared towards the effect of feature engineering on the methods (DeepMatcher used other features than PRLT) and on the question to what extent our simple heuristic was a good proxy to traditional programming methods implemented in existing entity matching systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mq8ao0EsksEo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "EntityMatchingFinal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
